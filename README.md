# ğŸ—ï¸ Kompletna Architektura Projektu - Document Processing API

## ğŸ“‹ Spis TreÅ›ci
1. [Struktura KatalogÃ³w](#struktura-katalogÃ³w)
2. [Diagramy Architektury](#diagramy-architektury)
3. [SzczegÃ³Å‚owa Architektura KomponentÃ³w](#szczegÃ³Å‚owa-architektura-komponentÃ³w)
4. [PrzepÅ‚ywy Danych](#przepÅ‚ywy-danych)
5. [Baza Danych](#baza-danych)
6. [Konfiguracja i Deployment](#konfiguracja-i-deployment)
7. [BezpieczeÅ„stwo](#bezpieczeÅ„stwo)
8. [Monitoring i ObserwabilnoÅ›Ä‡](#monitoring-i-obserwabilnoÅ›Ä‡)
9. [Skalowanie](#skalowanie)
10. [Troubleshooting](#troubleshooting)

---

## ğŸŒ³ Struktura KatalogÃ³w

```
document-processing-api/
â”‚
â”œâ”€â”€ ğŸ“ backend/
â”‚   â””â”€â”€ ğŸ“ app/
â”‚       â”œâ”€â”€ ğŸ“„ __init__.py
â”‚       â”œâ”€â”€ ğŸ“„ main.py                    # ğŸš€ FastAPI application entry point
â”‚       â”œâ”€â”€ ğŸ“„ celery_app.py              # ğŸ“‹ Celery configuration & task routing
â”‚       â”œâ”€â”€ ğŸ“„ database.py                # ğŸ—„ï¸ Database connection & session management
â”‚       â”œâ”€â”€ ğŸ“„ schemas.py                 # ğŸ“ Pydantic models for API validation
â”‚       â”œâ”€â”€ ğŸ“„ dependencies.py            # ğŸ”§ Dependency injection utilities
â”‚       â”œâ”€â”€ ğŸ“„ exceptions.py              # ğŸš¨ Custom exception handlers
â”‚       â”œâ”€â”€ ğŸ“„ config.py                  # âš™ï¸ Configuration management
â”‚       â”‚
â”‚       â”œâ”€â”€ ğŸ“ api/
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ documents.py           # ğŸ“„ Document-related endpoints
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ tasks.py              # ğŸ“‹ Task management endpoints
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ search.py             # ğŸ” Search & query endpoints
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ health.py             # ğŸ’š Health check endpoints
â”‚       â”‚   â””â”€â”€ ğŸ“„ analytics.py          # ğŸ“Š Analytics endpoints
â”‚       â”‚
â”‚       â”œâ”€â”€ ğŸ“ models/
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ base.py               # ğŸ—ï¸ Base model with common fields
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ document.py           # ğŸ“„ Document SQLAlchemy model
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ task.py               # ğŸ“‹ Task SQLAlchemy model
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ user.py               # ğŸ‘¤ User model (for auth)
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ audit.py              # ğŸ“ Audit trail model
â”‚       â”‚   â””â”€â”€ ğŸ“„ analytics.py          # ğŸ“Š Analytics & metrics models
â”‚       â”‚
â”‚       â”œâ”€â”€ ğŸ“ services/
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ document_service.py    # ğŸ“„ Document processing business logic
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ vector_service.py      # ğŸ§  Weaviate integration & vector operations
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ transcription_service.py # ğŸ¤ Audio transcription with Whisper
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ embedding_service.py   # ğŸ”¢ Embedding generation & management
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ search_service.py      # ğŸ” Search & similarity operations
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ notification_service.py # ğŸ“§ Notification management
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ cache_service.py       # ğŸ’¾ Caching strategies
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ audit_service.py       # ğŸ“ Audit logging service
â”‚       â”‚   â””â”€â”€ ğŸ“„ analytics_service.py   # ğŸ“Š Analytics & reporting
â”‚       â”‚
â”‚       â”œâ”€â”€ ğŸ“ workers/
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ document_processor.py  # ğŸ“„ Document processing worker
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ transcription.py       # ğŸ¤ Audio transcription worker
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ embedding_generator.py # ğŸ”¢ Embedding generation worker
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ batch_processor.py     # ğŸ“¦ Batch processing worker
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ cleanup_worker.py      # ğŸ§¹ Cleanup & maintenance tasks
â”‚       â”‚   â””â”€â”€ ğŸ“„ notification_worker.py # ğŸ“§ Notification dispatch worker
â”‚       â”‚
â”‚       â”œâ”€â”€ ğŸ“ core/
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ security.py           # ğŸ” Authentication & authorization
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ middleware.py         # ğŸ”„ Custom middleware
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ logging_config.py     # ğŸ“‹ Structured logging setup
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ metrics.py            # ğŸ“Š Application metrics
â”‚       â”‚   â””â”€â”€ ğŸ“„ rate_limiting.py      # ğŸš¦ Rate limiting implementation
â”‚       â”‚
â”‚       â”œâ”€â”€ ğŸ“ utils/
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ file_handler.py       # ğŸ“ File operations utilities
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ text_processor.py     # âœ‚ï¸ Text chunking & preprocessing
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ validators.py         # âœ… Custom validation functions
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ formatters.py         # ğŸ¨ Data formatting utilities
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ encryption.py         # ğŸ”’ Encryption utilities
â”‚       â”‚   â””â”€â”€ ğŸ“„ helpers.py            # ğŸ”§ General helper functions
â”‚       â”‚
â”‚       â””â”€â”€ ğŸ“ tests/
â”‚           â”œâ”€â”€ ğŸ“„ __init__.py
â”‚           â”œâ”€â”€ ğŸ“„ conftest.py           # ğŸ§ª Pytest configuration
â”‚           â”œâ”€â”€ ğŸ“ unit/                 # ğŸ”¬ Unit tests
â”‚           â”œâ”€â”€ ğŸ“ integration/          # ğŸ”„ Integration tests
â”‚           â”œâ”€â”€ ğŸ“ e2e/                 # ğŸŒ End-to-end tests
â”‚           â””â”€â”€ ğŸ“ fixtures/             # ğŸ“‹ Test data fixtures
â”‚
â”œâ”€â”€ ğŸ“ uploads/                          # ğŸ“‚ File upload directory
â”‚   â”œâ”€â”€ ğŸ“ documents/                    # ğŸ“„ Uploaded documents
â”‚   â”œâ”€â”€ ğŸ“ audio/                        # ğŸ¤ Audio files
â”‚   â”œâ”€â”€ ğŸ“ images/                       # ğŸ–¼ï¸ Image files
â”‚   â””â”€â”€ ğŸ“ temp/                         # ğŸ—‚ï¸ Temporary processing files
â”‚
â”œâ”€â”€ ğŸ“ scripts/                          # ğŸ”§ Utility scripts
â”‚   â”œâ”€â”€ ğŸ“„ setup_db.py                  # ğŸ—„ï¸ Database initialization
â”‚   â”œâ”€â”€ ğŸ“„ migrate_data.py              # ğŸ”„ Data migration scripts
â”‚   â”œâ”€â”€ ğŸ“„ cleanup_old_files.py         # ğŸ§¹ File cleanup utilities
â”‚   â”œâ”€â”€ ğŸ“„ backup_restore.py            # ğŸ’¾ Backup & restore tools
â”‚   â””â”€â”€ ğŸ“„ health_check.py              # ğŸ’š System health verification
â”‚
â”œâ”€â”€ ğŸ“ docs/                            # ğŸ“š Documentation
â”‚   â”œâ”€â”€ ğŸ“„ API_REFERENCE.md             # ğŸ“– API documentation
â”‚   â”œâ”€â”€ ğŸ“„ DEPLOYMENT.md                # ğŸš€ Deployment guide
â”‚   â”œâ”€â”€ ğŸ“„ ARCHITECTURE.md              # ğŸ—ï¸ Architecture overview
â”‚   â””â”€â”€ ğŸ“ diagrams/                    # ğŸ“Š Architecture diagrams
â”‚
â”œâ”€â”€ ğŸ“ monitoring/                      # ğŸ“Š Monitoring configuration
â”‚   â”œâ”€â”€ ğŸ“„ prometheus.yml               # ğŸ“ˆ Prometheus config
â”‚   â”œâ”€â”€ ğŸ“„ grafana-dashboard.json       # ğŸ“Š Grafana dashboard
â”‚   â””â”€â”€ ğŸ“„ alerts.yml                   # ğŸš¨ Alert rules
â”‚
â”œâ”€â”€ ğŸ“ nginx/                           # ğŸŒ Reverse proxy configuration
â”‚   â”œâ”€â”€ ğŸ“„ nginx.conf                   # âš™ï¸ Nginx main config
â”‚   â”œâ”€â”€ ğŸ“„ sites-enabled/               # ğŸŒ Virtual host configs
â”‚   â””â”€â”€ ğŸ“„ ssl/                         # ğŸ”’ SSL certificates
â”‚
â”œâ”€â”€ ğŸ“„ docker-compose.yml               # ğŸ³ Development environment
â”œâ”€â”€ ğŸ“„ docker-compose.prod.yml          # ğŸš€ Production environment
â”œâ”€â”€ ğŸ“„ docker-compose.monitoring.yml    # ğŸ“Š Monitoring stack
â”œâ”€â”€ ğŸ“„ Dockerfile                       # ğŸ³ Application container
â”œâ”€â”€ ğŸ“„ Dockerfile.worker                # ğŸ‘· Worker container
â”œâ”€â”€ ğŸ“„ requirements.txt                 # ğŸ Python dependencies
â”œâ”€â”€ ğŸ“„ requirements-dev.txt             # ğŸ› ï¸ Development dependencies
â”œâ”€â”€ ğŸ“„ .env.example                     # ğŸ“‹ Environment template
â”œâ”€â”€ ğŸ“„ .dockerignore                    # ğŸš« Docker ignore rules
â”œâ”€â”€ ğŸ“„ .gitignore                       # ğŸš« Git ignore rules
â”œâ”€â”€ ğŸ“„ Makefile                         # ğŸ”¨ Build automation
â”œâ”€â”€ ğŸ“„ pyproject.toml                   # ğŸ“¦ Python project config
â””â”€â”€ ğŸ“„ README.md                        # ğŸ“– Project documentation
```

---

## ğŸ—ï¸ Diagramy Architektury

### 1. ğŸŒ Architektura Systemu - Widok OgÃ³lny

```mermaid
graph TB
    subgraph "ğŸŒ External Layer"
        Users[ğŸ‘¥ Users]
        APIs[ğŸ”— External APIs]
        CDN[ğŸš€ CDN]
    end
    
    subgraph "ğŸ›¡ï¸ Security Layer"
        WAF[ğŸ›¡ï¸ Web Application Firewall]
        RateLimit[ğŸš¦ Rate Limiter]
        Auth[ğŸ” Authentication Service]
    end
    
    subgraph "ğŸŒ Load Balancer Layer"
        LB[âš–ï¸ Load Balancer<br/>Nginx + SSL]
        HealthCheck[ğŸ’š Health Check]
    end
    
    subgraph "ğŸš€ Application Layer"
        API1[ğŸ“± FastAPI App 1<br/>Port: 8001]
        API2[ğŸ“± FastAPI App 2<br/>Port: 8002]
        API3[ğŸ“± FastAPI App N<br/>Port: 800N]
        
        Worker1[ğŸ‘· Document Worker<br/>Queue: default]
        Worker2[ğŸ¤ Transcription Worker<br/>Queue: high_priority]
        Worker3[ğŸ”¢ Embedding Worker<br/>Queue: low_priority]
        WorkerN[ğŸ§¹ Cleanup Worker<br/>Queue: maintenance]
    end
    
    subgraph "ğŸ“¨ Message Queue Layer"
        Redis[(ğŸ”´ Redis Cluster<br/>Port: 6379)]
        Queue1[ğŸ“‹ Default Queue<br/>Document Processing]
        Queue2[âš¡ High Priority Queue<br/>Real-time Tasks]
        Queue3[ğŸŒ Low Priority Queue<br/>Background Tasks]
        Queue4[ğŸ› ï¸ Maintenance Queue<br/>Cleanup & Admin]
    end
    
    subgraph "ğŸ’¾ Storage Layer"
        Primary[(ğŸ—„ï¸ PostgreSQL Primary<br/>Port: 5432)]
        Replica1[(ğŸ“– PostgreSQL Replica 1<br/>Read Only)]
        Replica2[(ğŸ“– PostgreSQL Replica 2<br/>Read Only)]
        
        VectorPrimary[(ğŸ§  Weaviate Primary<br/>Port: 8080)]
        VectorReplica[(ğŸ§  Weaviate Replica<br/>Port: 8081)]
        
        Files[ğŸ“ File Storage<br/>S3 Compatible]
        Cache[ğŸ’¾ Redis Cache<br/>Port: 6380]
    end
    
    subgraph "ğŸ“Š Monitoring Layer"
        Prometheus[ğŸ“ˆ Prometheus]
        Grafana[ğŸ“Š Grafana Dashboard]
        AlertManager[ğŸš¨ Alert Manager]
        Jaeger[ğŸ” Jaeger Tracing]
    end
    
    Users --> CDN
    CDN --> WAF
    WAF --> RateLimit
    RateLimit --> Auth
    Auth --> LB
    
    LB --> API1
    LB --> API2
    LB --> API3
    
    API1 --> Redis
    API2 --> Redis
    API3 --> Redis
    
    API1 --> Primary
    API2 --> Replica1
    API3 --> Replica2
    
    API1 --> Cache
    API2 --> Cache
    API3 --> Cache
    
    Redis --> Queue1
    Redis --> Queue2
    Redis --> Queue3
    Redis --> Queue4
    
    Queue1 --> Worker1
    Queue2 --> Worker2
    Queue3 --> Worker3
    Queue4 --> WorkerN
    
    Worker1 --> Primary
    Worker2 --> Primary
    Worker3 --> VectorPrimary
    WorkerN --> Primary
    
    Worker1 --> Files
    Worker2 --> Files
    Worker3 --> Files
    
    Worker1 --> VectorPrimary
    Worker3 --> VectorReplica
    
    API1 --> Prometheus
    Worker1 --> Prometheus
    Prometheus --> Grafana
    Prometheus --> AlertManager
    
    API1 --> Jaeger
    Worker1 --> Jaeger
```

### 2. ğŸ”„ SzczegÃ³Å‚owy Flow Przetwarzania DokumentÃ³w

```mermaid
sequenceDiagram
    participant Client as ğŸ‘¤ Client
    participant LB as âš–ï¸ Load Balancer
    participant API as ğŸ“± FastAPI
    participant Auth as ğŸ” Auth Service
    participant DB as ğŸ—„ï¸ PostgreSQL
    participant Redis as ğŸ”´ Redis
    participant Worker as ğŸ‘· Worker
    participant Vector as ğŸ§  Weaviate
    participant Files as ğŸ“ File Storage
    participant Cache as ğŸ’¾ Cache
    participant Notify as ğŸ“§ Notification
    
    Note over Client: Document Upload Process
    
    Client->>+LB: POST /documents/upload
    LB->>+API: Forward Request
    API->>+Auth: Validate Token
    Auth->>-API: Token Valid âœ…
    
    API->>+Files: Save Uploaded File
    Files->>-API: File Saved âœ…
    
    API->>+DB: Create Document Record
    DB->>-API: Document Created (ID: 123)
    
    API->>+Redis: Queue Processing Task
    Note over Redis: Task queued based on file type<br/>PDF/DOCX â†’ default<br/>Audio â†’ high_priority<br/>Images â†’ low_priority
    Redis->>-API: Task Queued (Task ID: abc-123)
    
    API->>+Cache: Cache Document Metadata
    Cache->>-API: Cached âœ…
    
    API->>-LB: Return Response
    LB->>-Client: 201 Created<br/>{document_id: 123, task_id: "abc-123"}
    
    Note over Worker: Async Processing Begins
    
    Redis->>+Worker: Deliver Task
    Worker->>+DB: Update Status: PROCESSING
    DB->>-Worker: Updated âœ…
    
    Worker->>+Files: Read File Content
    Files->>-Worker: File Content
    
    alt Document Processing
        Worker->>Worker: Extract Text
        Worker->>Worker: Chunk Text (1000 chars, 200 overlap)
        Worker->>+Vector: Store Document Chunks
        Vector->>-Worker: Vectors Stored (weaviate_id: vec-123)
    else Audio Processing
        Worker->>Worker: Whisper Transcription
        Worker->>+DB: Store Transcription
        DB->>-Worker: Transcription Saved âœ…
    else Image Processing
        Worker->>Worker: Generate Image Embeddings
        Worker->>+Vector: Store Image Vectors
        Vector->>-Worker: Vectors Stored âœ…
    end
    
    Worker->>+DB: Update Document Status: PROCESSED
    DB->>-Worker: Status Updated âœ…
    
    Worker->>+DB: Create Task Success Record
    DB->>-Worker: Task Record Created âœ…
    
    Worker->>+Cache: Invalidate Document Cache
    Cache->>-Worker: Cache Invalidated âœ…
    
    Worker->>+Notify: Send Completion Notification
    Notify->>-Worker: Notification Queued âœ…
    
    Note over Client: Status Polling
    
    Client->>+LB: GET /documents/123
    LB->>+API: Forward Request
    API->>+Cache: Check Cache
    Cache->>-API: Cache Miss âŒ
    
    API->>+DB: Query Document Status
    DB->>-API: Document Data
    
    API->>+Cache: Update Cache (TTL: 5min)
    Cache->>-API: Cached âœ…
    
    API->>-LB: Return Status
    LB->>-Client: 200 OK<br/>{status: "processed", weaviate_id: "vec-123"}
```

### 3. ğŸ§© SzczegÃ³Å‚owa Architektura KomponentÃ³w

```mermaid
graph TB
    subgraph "ğŸŒ API Gateway Layer"
        Router[ğŸ›¤ï¸ FastAPI Router]
        Middleware[ğŸ”„ Middleware Stack]
        CORS[ğŸŒ CORS Handler]
        RateLimit[ğŸš¦ Rate Limiter]
        Auth[ğŸ” JWT Authenticator]
        Validation[âœ… Request Validator]
        Logging[ğŸ“‹ Request Logger]
    end
    
    subgraph "ğŸ“± API Endpoints"
        DocAPI[ğŸ“„ Documents API<br/>/api/v1/documents]
        TaskAPI[ğŸ“‹ Tasks API<br/>/api/v1/tasks]
        SearchAPI[ğŸ” Search API<br/>/api/v1/search]
        HealthAPI[ğŸ’š Health API<br/>/health]
        MetricsAPI[ğŸ“Š Metrics API<br/>/metrics]
        AdminAPI[âš™ï¸ Admin API<br/>/admin]
    end
    
    subgraph "ğŸ§  Business Logic Services"
        DocService[ğŸ“„ Document Service<br/>â€¢ File validation<br/>â€¢ Metadata extraction<br/>â€¢ Type detection]
        
        VectorService[ğŸ§  Vector Service<br/>â€¢ Embedding generation<br/>â€¢ Similarity search<br/>â€¢ Vector operations]
        
        TranscriptionService[ğŸ¤ Transcription Service<br/>â€¢ Audio preprocessing<br/>â€¢ Whisper integration<br/>â€¢ Language detection]
        
        SearchService[ğŸ” Search Service<br/>â€¢ Full-text search<br/>â€¢ Semantic search<br/>â€¢ Result ranking]
        
        CacheService[ğŸ’¾ Cache Service<br/>â€¢ Redis integration<br/>â€¢ TTL management<br/>â€¢ Cache invalidation]
        
        NotifyService[ğŸ“§ Notification Service<br/>â€¢ Email notifications<br/>â€¢ Webhook callbacks<br/>â€¢ Status updates]
    end
    
    subgraph "ğŸ“Š Data Access Layer"
        DocumentRepo[ğŸ“„ Document Repository<br/>â€¢ CRUD operations<br/>â€¢ Query builders<br/>â€¢ Relationships]
        
        TaskRepo[ğŸ“‹ Task Repository<br/>â€¢ Status tracking<br/>â€¢ Progress updates<br/>â€¢ Error handling]
        
        UserRepo[ğŸ‘¤ User Repository<br/>â€¢ Authentication<br/>â€¢ Authorization<br/>â€¢ Profile management]
        
        AnalyticsRepo[ğŸ“Š Analytics Repository<br/>â€¢ Usage metrics<br/>â€¢ Performance data<br/>â€¢ Reporting queries]
    end
    
    subgraph "ğŸ‘· Worker Processes"
        DocWorker[ğŸ“„ Document Processor<br/>â€¢ PDF extraction<br/>â€¢ Text chunking<br/>â€¢ Metadata parsing]
        
        AudioWorker[ğŸ¤ Audio Processor<br/>â€¢ Format conversion<br/>â€¢ Noise reduction<br/>â€¢ Transcription]
        
        EmbedWorker[ğŸ”¢ Embedding Generator<br/>â€¢ Vector creation<br/>â€¢ Dimensionality reduction<br/>â€¢ Index updating]
        
        BatchWorker[ğŸ“¦ Batch Processor<br/>â€¢ Bulk operations<br/>â€¢ Data migrations<br/>â€¢ Report generation]
        
        CleanupWorker[ğŸ§¹ Cleanup Worker<br/>â€¢ Temp file removal<br/>â€¢ Log rotation<br/>â€¢ Cache cleanup]
    end
    
    Router --> Middleware
    Middleware --> CORS
    CORS --> RateLimit
    RateLimit --> Auth
    Auth --> Validation
    Validation --> Logging
    
    Logging --> DocAPI
    Logging --> TaskAPI
    Logging --> SearchAPI
    Logging --> HealthAPI
    Logging --> MetricsAPI
    Logging --> AdminAPI
    
    DocAPI --> DocService
    TaskAPI --> DocService
    SearchAPI --> SearchService
    
    DocService --> VectorService
    DocService --> TranscriptionService
    SearchService --> VectorService
    
    DocService --> CacheService
    VectorService --> CacheService
    
    DocService --> NotifyService
    
    DocService --> DocumentRepo
    TaskAPI --> TaskRepo
    SearchAPI --> AnalyticsRepo
    
    DocumentRepo --> DocWorker
    TaskRepo --> AudioWorker
    DocumentRepo --> EmbedWorker
    AnalyticsRepo --> BatchWorker
    TaskRepo --> CleanupWorker
```

### 4. ğŸ“Š PrzepÅ‚yw Danych w Systemie

```mermaid
flowchart TD
    A[ğŸ“¤ File Upload] --> B{ğŸ” File Type Detection}
    
    B -->|ğŸ“„ PDF/DOCX/TXT| C[ğŸ“‹ Document Processing Queue]
    B -->|ğŸ¤ Audio Files| D[âš¡ High Priority Queue]
    B -->|ğŸ–¼ï¸ Images| E[ğŸŒ Low Priority Queue]
    B -->|ğŸ“¦ Archive Files| F[ğŸ“¦ Batch Processing Queue]
    
    C --> G[ğŸ“„ Document Processor]
    D --> H[ğŸ¤ Audio Processor]
    E --> I[ğŸ–¼ï¸ Image Processor]
    F --> J[ğŸ“¦ Batch Processor]
    
    G --> G1[ğŸ“ Text Extraction]
    G1 --> G2[âœ‚ï¸ Text Chunking]
    G2 --> G3[ğŸ”¢ Embedding Generation]
    G3 --> G4[ğŸ§  Vector Storage]
    
    H --> H1[ğŸ”Š Audio Preprocessing]
    H1 --> H2[ğŸ¤ Whisper Transcription]
    H2 --> H3[ğŸŒ Language Detection]
    H3 --> H4[ğŸ“ Text Post-processing]
    
    I --> I1[ğŸ–¼ï¸ Image Preprocessing]
    I1 --> I2[ğŸ‘ï¸ Feature Extraction]
    I2 --> I3[ğŸ”¢ Image Embeddings]
    I3 --> I4[ğŸ§  Vector Storage]
    
    J --> J1[ğŸ“¦ Archive Extraction]
    J1 --> J2[ğŸ“ File Enumeration]
    J2 --> J3[ğŸ”„ Recursive Processing]
    J3 --> A
    
    G4 --> K[ğŸ’¾ Cache Update]
    H4 --> K
    I4 --> K
    
    K --> L[ğŸ“§ Notification Dispatch]
    L --> M[âœ… Status Update: PROCESSED]
    
    M --> N{ğŸ” Quality Check}
    N -->|âœ… Pass| O[ğŸ‰ Processing Complete]
    N -->|âŒ Fail| P[ğŸ”„ Retry Processing]
    P --> C
    
    O --> Q[ğŸ“Š Analytics Update]
    Q --> R[ğŸ“ˆ Metrics Collection]
    
    subgraph "ğŸš¨ Error Handling"
        Error1[âŒ Extraction Failed]
        Error2[âŒ Transcription Failed]
        Error3[âŒ Embedding Failed]
        
        Error1 --> Retry1[ğŸ”„ Retry Logic]
        Error2 --> Retry2[ğŸ”„ Retry Logic]
        Error3 --> Retry3[ğŸ”„ Retry Logic]
        
        Retry1 --> DLQ[ğŸ’€ Dead Letter Queue]
        Retry2 --> DLQ
        Retry3 --> DLQ
        
        DLQ --> Manual[ğŸ‘¨â€ğŸ’» Manual Review]
    end
    
    G1 --> Error1
    H2 --> Error2
    G3 --> Error3
```

---

## ğŸ“š SzczegÃ³Å‚owa Architektura KomponentÃ³w

### ğŸ”§ Core Services

#### ğŸ“„ Document Service
```python
class DocumentService:
    """
    GÅ‚Ã³wny serwis do zarzÄ…dzania dokumentami
    
    FunkcjonalnoÅ›ci:
    â€¢ Walidacja i detekcja typu plikÃ³w
    â€¢ Ekstrakcja metadanych (autor, data utworzenia, rozmiar)
    â€¢ Konwersja formatÃ³w dokumentÃ³w
    â€¢ Generowanie miniatur i podglÄ…dÃ³w
    â€¢ ZarzÄ…dzanie cyklem Å¼ycia dokumentu
    """
    
    SUPPORTED_FORMATS = {
        'documents': ['.pdf', '.docx', '.doc', '.txt', '.rtf', '.odt'],
        'audio': ['.mp3', '.wav', '.ogg', '.m4a', '.flac'],
        'images': ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.svg'],
        'archives': ['.zip', '.tar', '.gz', '.rar', '.7z']
    }
    
    MAX_FILE_SIZE = {
        'documents': 100 * 1024 * 1024,  # 100MB
        'audio': 500 * 1024 * 1024,      # 500MB
        'images': 50 * 1024 * 1024,      # 50MB
        'archives': 1024 * 1024 * 1024   # 1GB
    }
```

#### ğŸ§  Vector Service
```python
class VectorService:
    """
    Serwis do zarzÄ…dzania wektorami i operacjami wyszukiwania
    
    FunkcjonalnoÅ›ci:
    â€¢ Generowanie embeddings dla tekstu i obrazÃ³w
    â€¢ Przechowywanie wektorÃ³w w Weaviate
    â€¢ Wyszukiwanie semantyczne
    â€¢ Klastrowanie podobnych dokumentÃ³w
    â€¢ Rekomendacje na podstawie podobieÅ„stwa
    """
    
    EMBEDDING_MODELS = {
        'text': 'all-MiniLM-L6-v2',      # 384 dimensions
        'multilingual': 'paraphrase-multilingual-MiniLM-L12-v2',
        'image': 'clip-ViT-B-32',        # 512 dimensions
        'code': 'microsoft/codebert-base' # 768 dimensions
    }
    
    SIMILARITY_THRESHOLD = 0.7
    MAX_RESULTS = 100
```

#### ğŸ¤ Transcription Service
```python
class TranscriptionService:
    """
    Serwis transkrypcji audio przy uÅ¼yciu OpenAI Whisper
    
    FunkcjonalnoÅ›ci:
    â€¢ Automatyczna detekcja jÄ™zyka
    â€¢ Transkrypcja z znacznikami czasu
    â€¢ Identyfikacja gÅ‚oÅ›nikÃ³w (speaker diarization)
    â€¢ Filtrowanie szumÃ³w i normalizacja audio
    â€¢ Generowanie podsumowaÅ„ transkrypcji
    """
    
    WHISPER_MODELS = {
        'tiny': {'size': '39MB', 'speed': 'fastest', 'accuracy': 'lowest'},
        'base': {'size': '74MB', 'speed': 'fast', 'accuracy': 'medium'},
        'small': {'size': '244MB', 'speed': 'medium', 'accuracy': 'good'},
        'medium': {'size': '769MB', 'speed': 'slow', 'accuracy': 'better'},
        'large': {'size': '1550MB', 'speed': 'slowest', 'accuracy': 'best'}
    }
    
    SUPPORTED_LANGUAGES = ['en', 'pl', 'de', 'fr', 'es', 'it', 'ru', 'zh']
```

### ğŸ‘· Workers Architecture

#### ğŸ“„ Document Processing Worker
```mermaid
flowchart TD
    A[ğŸ“¥ Task Received] --> B[ğŸ“‹ Validate Input]
    B --> C{ğŸ“„ File Type?}
    
    C -->|PDF| D[ğŸ“• PDF Processor]
    C -->|DOCX| E[ğŸ“˜ Word Processor]
    C -->|TXT| F[ğŸ“„ Text Processor]
    C -->|HTML| G[ğŸŒ HTML Processor]
    
    D --> D1[ğŸ” Extract Text & Images]
    D1 --> D2[ğŸ“Š Extract Metadata]
    D2 --> D3[ğŸ¨ Generate Thumbnails]
    
    E --> E1[ğŸ“ Extract Text & Tables]
    E1 --> E2[ğŸ“Š Extract Styles & Formatting]
    E2 --> E3[ğŸ–¼ï¸ Extract Embedded Media]
    
    F --> F1[âœ‚ï¸ Text Chunking]
    G --> G1[ğŸ·ï¸ HTML Parsing & Cleanup]
    
    D3 --> H[âœ‚ï¸ Text Chunking]
    E3 --> H
    F1 --> H
    G1 --> H
    
    H --> I[ğŸ”¢ Generate Embeddings]
    I --> J[ğŸ§  Store in Weaviate]
    J --> K[ğŸ’¾ Update Database]
    K --> L[ğŸ“§ Send Notification]
    L --> M[âœ… Task Complete]
    
    subgraph "ğŸš¨ Error Handling"
        N[âŒ Processing Error]
        O[ğŸ”„ Retry Logic<br/>Max: 3 attempts]
        P[ğŸ’€ Dead Letter Queue]
        
        N --> O
        O -->|Failed| P
        O -->|Success| H
    end
    
    D --> N
    E --> N
    F --> N
    G --> N
```

#### ğŸ¤ Audio Processing Worker
```mermaid
flowchart TD
    A[ğŸ“¥ Audio Task] --> B[ğŸ”Š Audio Validation]
    B --> C[ğŸ“Š Analyze Audio Properties]
    
    C --> D{ğŸµ Audio Quality?}
    D -->|Good| E[ğŸ¤ Direct Transcription]
    D -->|Poor| F[ğŸ”§ Audio Enhancement]
    
    F --> F1[ğŸ”‡ Noise Reduction]
    F1 --> F2[ğŸ“¢ Volume Normalization]
    F2 --> F3[ğŸ›ï¸ Format Conversion]
    F3 --> E
    
    E --> G[ğŸŒ Language Detection]
    G --> H{ğŸ—£ï¸ Multiple Speakers?}
    
    H -->|Yes| I[ğŸ‘¥ Speaker Diarization]
    H -->|No| J[ğŸ¤ Single Speaker Transcription]
    
    I --> I1[ğŸ” Voice Activity Detection]
    I1 --> I2[ğŸ‘¤ Speaker Identification]
    I2 --> I3[â±ï¸ Timestamp Alignment]
    I3 --> K[ğŸ“ Multi-Speaker Transcript]
    
    J --> J1[ğŸ¤ Whisper Transcription]
    J1 --> J2[â±ï¸ Timestamp Generation]
    J2 --> L[ğŸ“ Single Speaker Transcript]
    
    K --> M[ğŸ” Post-Processing]
    L --> M
    
    M --> M1[âœï¸ Grammar Correction]
    M1 --> M2[ğŸ”¤ Spell Check]
    M2 --> M3[ğŸ“– Punctuation Enhancement]
    M3 --> N[ğŸ’¾ Store Transcription]
    
    N --> O[ğŸ”¢ Generate Text Embeddings]
    O --> P[ğŸ§  Store in Vector DB]
    P --> Q[ğŸ“Š Update Analytics]
    Q --> R[âœ… Task Complete]
    
    subgraph "ğŸš¨ Audio Processing Errors"
        S[âŒ Transcription Failed]
        T[ğŸ”„ Retry with Different Model]
        U[ğŸ“ Human Review Queue]
        
        S --> T
        T -->|Max Retries| U
        T -->|Success| M
    end
    
    E --> S
    J1 --> S

#### ğŸ”¢ Embedding Generation Worker
```mermaid
flowchart TD
    A[ğŸ“¥ Embedding Task] --> B[ğŸ“Š Analyze Content Type]
    B --> C{ğŸ“ Content Type?}
    
    C -->|Text| D[ğŸ“„ Text Embedding Pipeline]
    C -->|Image| E[ğŸ–¼ï¸ Image Embedding Pipeline]
    C -->|Code| F[ğŸ’» Code Embedding Pipeline]
    C -->|Audio| G[ğŸ¤ Audio Feature Extraction]
    
    D --> D1[ğŸ”¤ Text Preprocessing]
    D1 --> D2[âœ‚ï¸ Text Chunking]
    D2 --> D3[ğŸ§  Generate Text Vectors]
    D3 --> H[ğŸ” Quality Validation]
    
    E --> E1[ğŸ–¼ï¸ Image Preprocessing]
    E1 --> E2[ğŸ¨ Feature Extraction]
    E2 --> E3[ğŸ§  Generate Image Vectors]
    E3 --> H
    
    F --> F1[ğŸ’» Code Analysis]
    F1 --> F2[ğŸ·ï¸ Syntax Highlighting]
    F2 --> F3[ğŸ§  Generate Code Vectors]
    F3 --> H
    
    G --> G1[ğŸµ Audio Feature Analysis]
    G1 --> G2[ğŸ“Š Spectral Analysis]
    G2 --> G3[ğŸ§  Generate Audio Vectors]
    G3 --> H
    
    H --> I{âœ… Quality OK?}
    I -->|Yes| J[ğŸ§  Store in Vector DB]
    I -->|No| K[ğŸ”„ Regenerate with Different Model]
    
    K --> L{ğŸ”¢ Retry Count < 3?}
    L -->|Yes| D
    L -->|No| M[âŒ Mark as Failed]
    
    J --> N[ğŸ·ï¸ Create Vector Index]
    N --> O[ğŸ”— Link to Original Document]
    O --> P[ğŸ“Š Update Metadata]
    P --> Q[âœ… Task Complete]
    
    M --> R[ğŸ“§ Admin Notification]
    R --> S[ğŸ“ Error Log]
```

---

## ğŸ’¾ SzczegÃ³Å‚owa Architektura Bazy Danych

### ğŸ—„ï¸ PostgreSQL Schema Design

```mermaid
erDiagram
    USERS {
        uuid id PK
        string email UK
        string username UK
        string password_hash
        string first_name
        string last_name
        enum role
        jsonb preferences
        boolean is_active
        datetime last_login
        datetime created_at
        datetime updated_at
        string created_by FK
        string updated_by FK
    }
    
    DOCUMENTS {
        bigint id PK
        uuid user_id FK
        string filename
        string original_filename
        string file_path
        string content_type
        bigint file_size
        string file_hash
        enum status
        string task_id
        string weaviate_id
        text transcription
        jsonb metadata
        jsonb processing_config
        integer chunk_count
        float processing_time
        datetime uploaded_at
        datetime processed_at
        datetime created_at
        datetime updated_at
        string created_by FK
        string updated_by FK
    }
    
    DOCUMENT_CHUNKS {
        bigint id PK
        bigint document_id FK
        integer chunk_index
        text content
        integer start_position
        integer end_position
        float confidence_score
        jsonb metadata
        string weaviate_chunk_id
        datetime created_at
        datetime updated_at
    }
    
    TASKS {
        bigint id PK
        string task_id UK
        uuid user_id FK
        bigint document_id FK
        string task_type
        enum status
        enum priority
        jsonb input_data
        jsonb result_data
        jsonb progress_data
        text error_message
        string error_code
        integer retry_count
        integer max_retries
        datetime started_at
        datetime completed_at
        datetime next_retry_at
        float processing_time
        string worker_id
        datetime created_at
        datetime updated_at
    }
    
    SEARCH_QUERIES {
        bigint id PK
        uuid user_id FK
        text query
        jsonb filters
        jsonb results_metadata
        integer results_count
        float response_time
        enum search_type
        datetime created_at
    }
    
    AUDIT_LOGS {
        bigint id PK
        uuid user_id FK
        string entity_type
        string entity_id
        string action
        jsonb old_values
        jsonb new_values
        string ip_address
        string user_agent
        datetime created_at
    }
    
    ANALYTICS_EVENTS {
        bigint id PK
        uuid user_id FK
        string event_type
        string entity_type
        string entity_id
        jsonb event_data
        string session_id
        string ip_address
        datetime created_at
    }
    
    NOTIFICATIONS {
        bigint id PK
        uuid user_id FK
        bigint document_id FK
        bigint task_id FK
        string type
        string title
        text message
        jsonb data
        boolean is_read
        datetime read_at
        datetime created_at
        datetime updated_at
    }
    
    API_KEYS {
        bigint id PK
        uuid user_id FK
        string key_hash
        string name
        jsonb permissions
        integer rate_limit
        datetime last_used_at
        datetime expires_at
        boolean is_active
        datetime created_at
        datetime updated_at
    }
    
    SYSTEM_METRICS {
        bigint id PK
        string metric_name
        string metric_type
        float value
        jsonb labels
        datetime timestamp
        datetime created_at
    }
    
    USERS ||--o{ DOCUMENTS : "owns"
    USERS ||--o{ TASKS : "initiates"
    USERS ||--o{ SEARCH_QUERIES : "performs"
    USERS ||--o{ AUDIT_LOGS : "generates"
    USERS ||--o{ ANALYTICS_EVENTS : "triggers"
    USERS ||--o{ NOTIFICATIONS : "receives"
    USERS ||--o{ API_KEYS : "has"
    
    DOCUMENTS ||--o{ DOCUMENT_CHUNKS : "contains"
    DOCUMENTS ||--o{ TASKS : "processes"
    DOCUMENTS ||--o{ NOTIFICATIONS : "relates_to"
    
    TASKS ||--o{ NOTIFICATIONS : "generates"
```

### ğŸ“Š Database Indexes Strategy

```sql
-- Performance Critical Indexes
CREATE INDEX CONCURRENTLY idx_documents_user_status 
ON documents(user_id, status) 
WHERE status IN ('pending', 'processing');

CREATE INDEX CONCURRENTLY idx_documents_content_type 
ON documents(content_type, created_at DESC);

CREATE INDEX CONCURRENTLY idx_tasks_status_priority 
ON tasks(status, priority, created_at) 
WHERE status IN ('pending', 'processing');

CREATE INDEX CONCURRENTLY idx_document_chunks_document_embedding 
ON document_chunks(document_id, weaviate_chunk_id) 
WHERE weaviate_chunk_id IS NOT NULL;

-- Full Text Search Indexes
CREATE INDEX CONCURRENTLY idx_documents_search 
ON documents USING gin(to_tsvector('english', filename || ' ' || COALESCE(metadata->>'title', '')));

CREATE INDEX CONCURRENTLY idx_document_chunks_search 
ON document_chunks USING gin(to_tsvector('english', content));

-- Analytics Indexes
CREATE INDEX CONCURRENTLY idx_analytics_events_user_time 
ON analytics_events(user_id, created_at DESC);

CREATE INDEX CONCURRENTLY idx_search_queries_performance 
ON search_queries(created_at DESC, response_time);

-- Audit Indexes
CREATE INDEX CONCURRENTLY idx_audit_logs_entity 
ON audit_logs(entity_type, entity_id, created_at DESC);

-- Partitioning Strategy for Large Tables
CREATE TABLE analytics_events_y2024m01 PARTITION OF analytics_events 
FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');

CREATE TABLE audit_logs_y2024m01 PARTITION OF audit_logs 
FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');
```

---

## ğŸ§  Weaviate Vector Database Architecture

### ğŸ“š Schema Definition

```python
WEAVIATE_SCHEMA = {
    "classes": [
        {
            "class": "Document",
            "description": "Processed document with metadata and embeddings",
            "vectorizer": "text2vec-transformers",
            "moduleConfig": {
                "text2vec-transformers": {
                    "poolingStrategy": "masked_mean",
                    "model": "sentence-transformers/all-MiniLM-L6-v2"
                },
                "generative-openai": {
                    "model": "gpt-3.5-turbo"
                }
            },
            "properties": [
                {
                    "name": "content",
                    "dataType": ["text"],
                    "description": "Full document content"
                },
                {
                    "name": "title",
                    "dataType": ["string"],
                    "description": "Document title"
                },
                {
                    "name": "author",
                    "dataType": ["string"],
                    "description": "Document author"
                },
                {
                    "name": "document_type",
                    "dataType": ["string"],
                    "description": "Type of document (pdf, docx, etc.)"
                },
                {
                    "name": "language",
                    "dataType": ["string"],
                    "description": "Detected language"
                },
                {
                    "name": "created_date",
                    "dataType": ["date"],
                    "description": "Document creation date"
                },
                {
                    "name": "file_size",
                    "dataType": ["int"],
                    "description": "File size in bytes"
                },
                {
                    "name": "tags",
                    "dataType": ["string[]"],
                    "description": "Document tags"
                },
                {
                    "name": "user_id",
                    "dataType": ["string"],
                    "description": "Owner user ID"
                },
                {
                    "name": "postgres_id",
                    "dataType": ["string"],
                    "description": "Reference to PostgreSQL record"
                }
            ]
        },
        {
            "class": "DocumentChunk",
            "description": "Individual chunks of larger documents",
            "vectorizer": "text2vec-transformers",
            "moduleConfig": {
                "text2vec-transformers": {
                    "poolingStrategy": "masked_mean",
                    "model": "sentence-transformers/all-MiniLM-L6-v2"
                }
            },
            "properties": [
                {
                    "name": "content",
                    "dataType": ["text"],
                    "description": "Chunk content"
                },
                {
                    "name": "chunk_index",
                    "dataType": ["int"],
                    "description": "Index within document"
                },
                {
                    "name": "start_position",
                    "dataType": ["int"],
                    "description": "Start character position"
                },
                {
                    "name": "end_position",
                    "dataType": ["int"],
                    "description": "End character position"
                },
                {
                    "name": "confidence_score",
                    "dataType": ["number"],
                    "description": "Extraction confidence"
                },
                {
                    "name": "parent_document",
                    "dataType": ["Document"],
                    "description": "Reference to parent document"
                },
                {
                    "name": "postgres_id",
                    "dataType": ["string"],
                    "description": "Reference to PostgreSQL record"
                }
            ]
        },
        {
            "class": "AudioTranscript",
            "description": "Transcribed audio content with speaker information",
            "vectorizer": "text2vec-transformers",
            "properties": [
                {
                    "name": "transcript",
                    "dataType": ["text"],
                    "description": "Full transcript text"
                },
                {
                    "name": "language",
                    "dataType": ["string"],
                    "description": "Detected language"
                },
                {
                    "name": "confidence",
                    "dataType": ["number"],
                    "description": "Transcription confidence"
                },
                {
                    "name": "speakers",
                    "dataType": ["string[]"],
                    "description": "Identified speakers"
                },
                {
                    "name": "duration",
                    "dataType": ["number"],
                    "description": "Audio duration in seconds"
                },
                {
                    "name": "timestamps",
                    "dataType": ["string"],
                    "description": "JSON with word-level timestamps"
                },
                {
                    "name": "parent_document",
                    "dataType": ["Document"],
                    "description": "Reference to source audio file"
                }
            ]
        },
        {
            "class": "ImageEmbedding",
            "description": "Visual features and metadata for images",
            "vectorizer": "img2vec-neural",
            "moduleConfig": {
                "img2vec-neural": {
                    "imageFields": ["image"],
                    "model": "resnet50"
                }
            },
            "properties": [
                {
                    "name": "image",
                    "dataType": ["blob"],
                    "description": "Base64 encoded image"
                },
                {
                    "name": "description",
                    "dataType": ["text"],
                    "description": "Image description/caption"
                },
                {
                    "name": "width",
                    "dataType": ["int"],
                    "description": "Image width in pixels"
                },
                {
                    "name": "height",
                    "dataType": ["int"],
                    "description": "Image height in pixels"
                },
                {
                    "name": "format",
                    "dataType": ["string"],
                    "description": "Image format (jpg, png, etc.)"
                },
                {
                    "name": "dominant_colors",
                    "dataType": ["string[]"],
                    "description": "Dominant colors in image"
                },
                {
                    "name": "objects_detected",
                    "dataType": ["string[]"],
                    "description": "Detected objects in image"
                },
                {
                    "name": "parent_document",
                    "dataType": ["Document"],
                    "description": "Reference to source document"
                }
            ]
        }
    ]
}
```

### ğŸ” Advanced Search Capabilities

```python
class AdvancedSearchService:
    """
    Zaawansowane moÅ¼liwoÅ›ci wyszukiwania w Weaviate
    """
    
    def semantic_search(self, query: str, filters: dict = None, limit: int = 10):
        """Wyszukiwanie semantyczne z filtrami"""
        where_filter = self._build_where_filter(filters)
        
        result = (
            self.client.query
            .get("DocumentChunk", [
                "content", "chunk_index", "confidence_score",
                "parent_document { ... on Document { title, author, document_type } }"
            ])
            .with_near_text({
                "concepts": [query],
                "distance": 0.7
            })
            .with_where(where_filter)
            .with_limit(limit)
            .with_additional(["distance", "certainty"])
            .do()
        )
        
        return self._process_search_results(result)
    
    def hybrid_search(self, query: str, alpha: float = 0.5, limit: int = 10):
        """Hybrydowe wyszukiwanie (semantyczne + BM25)"""
        result = (
            self.client.query
            .get("DocumentChunk", [
                "content", "chunk_index",
                "parent_document { ... on Document { title, author } }"
            ])
            .with_hybrid(query, alpha=alpha)
            .with_limit(limit)
            .with_additional(["score"])
            .do()
        )
        
        return self._process_search_results(result)
    
    def multi_modal_search(self, text_query: str = None, image_query: str = None):
        """Wyszukiwanie multimodalne (tekst + obraz)"""
        concepts = []
        if text_query:
            concepts.append({"text": text_query})
        if image_query:
            concepts.append({"image": image_query})
            
        result = (
            self.client.query
            .get("Document", ["title", "content", "document_type"])
            .with_near_text({"concepts": concepts})
            .with_limit(20)
            .do()
        )
        
        return result
    
    def find_similar_documents(self, document_id: str, limit: int = 5):
        """ZnajdÅº podobne dokumenty"""
        # Pobierz wektor dokumentu ÅºrÃ³dÅ‚owego
        source_doc = self.client.query.get("Document", ["_additional { vector }"]).with_where({
            "path": ["postgres_id"],
            "operator": "Equal",
            "valueString": document_id
        }).do()
        
        if not source_doc["data"]["Get"]["Document"]:
            return []
            
        source_vector = source_doc["data"]["Get"]["Document"][0]["_additional"]["vector"]
        
        # ZnajdÅº podobne dokumenty
        result = (
            self.client.query
            .get("Document", ["title", "author", "document_type", "postgres_id"])
            .with_near_vector({
                "vector": source_vector,
                "distance": 0.8
            })
            .with_limit(limit + 1)  # +1 bo ÅºrÃ³dÅ‚owy dokument teÅ¼ bÄ™dzie w wynikach
            .with_additional(["distance"])
            .do()
        )
        
        # UsuÅ„ ÅºrÃ³dÅ‚owy dokument z wynikÃ³w
        documents = result["data"]["Get"]["Document"]
        return [doc for doc in documents if doc["postgres_id"] != document_id]
```

---

## ğŸš€ Konfiguracja i Deployment

### ğŸ³ Docker Configuration

#### Production Docker Compose
```yaml
version: '3.8'

services:
  # Load Balancer
  nginx:
    image: nginx:1.25-alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/sites-enabled:/etc/nginx/sites-enabled:ro
      - ./ssl:/etc/nginx/ssl:ro
      - ./logs/nginx:/var/log/nginx
    depends_on:
      - app1
      - app2
      - app3
    restart: unless-stopped
    networks:
      - frontend
      - backend
    deploy:
      resources:
        limits:
          memory: 512M
        reservations:
          memory: 256M

  # FastAPI Applications
  app1: &app
    build:
      context: .
      dockerfile: Dockerfile
      target: production
    environment: &app_env
      DATABASE_URL: postgresql://postgres:${POSTGRES_PASSWORD}@postgres-primary:5432/${POSTGRES_DB}
      DATABASE_REPLICA_URL: postgresql://postgres:${POSTGRES_PASSWORD}@postgres-replica:5432/${POSTGRES_DB}
      CELERY_BROKER_URL: redis://redis-cluster:6379/0
      CELERY_RESULT_BACKEND: redis://redis-cluster:6379/1
      WEAVIATE_URL: http://weaviate:8080
      WEAVIATE_API_KEY: ${WEAVIATE_API_KEY}
      JWT_SECRET_KEY: ${JWT_SECRET_KEY}
      ENVIRONMENT: production
      LOG_LEVEL: INFO
      SENTRY_DSN: ${SENTRY_DSN}
      S3_BUCKET: ${S3_BUCKET}
      S3_ACCESS_KEY: ${S3_ACCESS_KEY}
      S3_SECRET_KEY: ${S3_SECRET_KEY}
      S3_ENDPOINT: ${S3_ENDPOINT}
      OPENAI_API_KEY: ${OPENAI_API_KEY}
    volumes:
      - uploads:/app/uploads
      - logs:/app/logs
    depends_on:
      postgres-primary:
        condition: service_healthy
      redis-cluster:
        condition: service_healthy
      weaviate:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - backend
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 1G
          cpus: '0.5'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  app2:
    <<: *app
    
  app3:
    <<: *app

  # Celery Workers
  worker-default: &worker
    build:
      context: .
      dockerfile: Dockerfile.worker
      target: production
    command: celery -A app.celery_app worker --loglevel=info -Q default --concurrency=4 --prefetch-multiplier=1
    environment: *app_env
    volumes:
      - uploads:/app/uploads
      - logs:/app/logs
      - worker-temp:/tmp
    depends_on:
      postgres-primary:
        condition: service_healthy
      redis-cluster:
        condition: service_healthy
      weaviate:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - backend
    deploy:
      replicas: 3
      resources:
        limits:
          memory: 4G
          cpus: '2.0'
        reservations:
          memory: 2G
          cpus: '1.0'
    healthcheck:
      test: ["CMD", "celery", "-A", "app.celery_app", "inspect", "ping"]
      interval: 60s
      timeout: 10s
      retries: 3

  worker-high-priority:
    <<: *worker
    command: celery -A app.celery_app worker --loglevel=info -Q high_priority --concurrency=2
    deploy:
      replicas: 2
      resources:
        limits:
          memory: 8G
          cpus: '4.0'
        reservations:
          memory: 4G
          cpus: '2.0'

  worker-low-priority:
    <<: *worker
    command: celery -A app.celery_app worker --loglevel=info -Q low_priority --concurrency=8
    deploy:
      replicas: 2
      resources:
        limits:
          memory: 6G
          cpus: '3.0'
        reservations:
          memory: 3G
          cpus: '1.5'

  # Celery Beat Scheduler
  celery-beat:
    build:
      context: .
      dockerfile: Dockerfile.worker
    command: celery -A app.celery_app beat --loglevel=info --scheduler django_celery_beat.schedulers:DatabaseScheduler
    environment: *app_env
    volumes:
      - logs:/app/logs
    depends_on:
      postgres-primary:
        condition: service_healthy
      redis-cluster:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - backend

  # Celery Flower (Monitoring)
  flower:
    build:
      context: .
      dockerfile: Dockerfile.worker
    command: celery -A app.celery_app flower --port=5555
    ports:
      - "5555:5555"
    environment: *app_env
    depends_on:
      - redis-cluster
    restart: unless-stopped
    networks:
      - backend
      - monitoring

  # Database Primary
  postgres-primary:
    image: postgres:15-alpine
    environment:
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_REPLICATION_USER: replicator
      POSTGRES_REPLICATION_PASSWORD: ${POSTGRES_REPLICATION_PASSWORD}
    ports:
      - "5432:5432"
    volumes:
      - postgres_primary_data:/var/lib/postgresql/data
      - ./scripts/postgres/init:/docker-entrypoint-initdb.d
      - ./postgres/postgresql.conf:/etc/postgresql/postgresql.conf
      - postgres_logs:/var/log/postgresql
    command: |
      postgres
        -c config_file=/etc/postgresql/postgresql.conf
        -c log_destination=stderr
        -c logging_collector=on
        -c log_directory=/var/log/postgresql
        -c log_filename=postgresql-%Y-%m-%d.log
    restart: unless-stopped
    networks:
      - backend
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2.0'
        reservations:
          memory: 2G
          cpus: '1.0'
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres -d ${POSTGRES_DB}"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Database Replica (Read-only)
  postgres-replica:
    image: postgres:15-alpine
    environment:
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      PGUSER: postgres
    volumes:
      - postgres_replica_data:/var/lib/postgresql/data
      - ./scripts/postgres/replica:/docker-entrypoint-initdb.d
    command: |
      postgres
        -c wal_level=replica
        -c max_wal_senders=3
        -c wal_keep_segments=64
    depends_on:
      postgres-primary:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - backend
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres -d ${POSTGRES_DB}"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Redis Cluster
  redis-cluster:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    command: |
      redis-server
        --appendonly yes
        --maxmemory 2gb
        --maxmemory-policy allkeys-lru
        --save 900 1
        --save 300 10
        --save 60 10000
    volumes:
      - redis_data:/data
      - ./redis/redis.conf:/etc/redis/redis.conf
    restart: unless-stopped
    networks:
      - backend
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 1G
          cpus: '0.5'
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Cache Redis (Separate instance)
  redis-cache:
    image: redis:7-alpine
    ports:
      - "6380:6379"
    command: |
      redis-server
        --maxmemory 1gb
        --maxmemory-policy allkeys-lru
        --save ""
    volumes:
      - redis_cache_data:/data
    restart: unless-stopped
    networks:
      - backend
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Weaviate Vector Database
  weaviate:
    image: semitechnologies/weaviate:1.22.4
    ports:
      - "8080:8080"
    environment:
      QUERY_DEFAULTS_LIMIT: 25
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'false'
      AUTHENTICATION_APIKEY_ENABLED: 'true'
      AUTHENTICATION_APIKEY_ALLOWED_KEYS: ${WEAVIATE_API_KEY}
      AUTHENTICATION_APIKEY_USERS: 'admin'
      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'
      DEFAULT_VECTORIZER_MODULE: 'text2vec-transformers'
      ENABLE_MODULES: 'text2vec-transformers,text2vec-openai,generative-openai,img2vec-neural'
      TRANSFORMERS_INFERENCE_API: 'http://t2v-transformers:8080'
      CLUSTER_HOSTNAME: 'node1'
      CLUSTER_GOSSIP_BIND_PORT: '7100'
      CLUSTER_DATA_BIND_PORT: '7101'
    volumes
